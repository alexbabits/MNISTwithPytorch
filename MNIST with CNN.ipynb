{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from PIL import Image (Helps add image processing if needed)\n",
    "#from torch.utils.data import Dataset (for custom datasets if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "# Number of classes in the MNIST dataset (digits 0-9).\n",
    "num_classes = 10\n",
    "# (channels, [dimensions of image in pixels]). Greyscale = 1, RGB = 3.\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# Instances of Pytorch dataset objects which load and split the MNIST data into train and test sets and transforms the images to tensors.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "# Objects to provide access to the tensor data during training and testing.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=676, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Building the model (Creating custom CNN 'Net' using nn.Module class)\n",
    "class Net(nn.Module):\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "        # Constructor which initializes the layers and defines parameters.\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer for extracting local features/patterns. (input channels, output channels, kernel size, stride).\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
    "        # max pooling layer for downsampling to retain important features & reduce spatial dimensions. (kernel size, stride).\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # dropout layer for regularization. Prevents overfitting. (Probability of element dropped during dropout).\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        # fully connected layer for classification. (input size, output size).\n",
    "        self.fc1 = nn.Linear(4 * 13 * 13, num_classes)\n",
    "\n",
    "    # Function to define how the input flows through the layers.\n",
    "    def forward(self, x):\n",
    "        # Applies convolutional layer, activation function (ReLU) for non-linearity, and pooling layer to the input.\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Reshapes the output tensor to match the input size of the fully connected layer.\n",
    "        x = x.view(-1, 4 * 13 * 13)\n",
    "        # Applies dropout layer to prevent overfitting by randomly zeroing some elements.\n",
    "        x = self.dropout(x)\n",
    "        # Flattens and passes the input through the fully connected layer, performs linear transformation for classification.\n",
    "        x = self.fc1(x)\n",
    "        # Logarithmic softmax function applied to produce the output probabilities.\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer using Adam op algo to optimize parameters during training with the set learning rate.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Loss function (combines softmax activation and negative log-likelihood loss).\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Performs the training loop for a number of epochs.\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Sets model in training mode, so it can update it's parameters during training.\n",
    "    model.train()\n",
    "    # Iterates over the loader.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Input data and target labels moved to the GPU/CPU.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Optimizer's gradients reset to clear accumulated gradients from pervious batches.\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass is performed, obtaining predicted outputs.\n",
    "        output = model(data)\n",
    "        # Loss calculated by comparing predicted outputs with target labels.\n",
    "        loss = criterion(output, target)\n",
    "        # Gradients are computed and backproped.\n",
    "        loss.backward()\n",
    "        # Optimizer updates models parameters.\n",
    "        optimizer.step()\n",
    "        # Prints current training process when we hit a round number. Loss.item() is scalar value of the loss tensor.\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297239\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.195651\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.783767\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.377728\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.475029\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.416796\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.376504\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.239490\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584582\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.293359\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.452876\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.249632\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.810044\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.356825\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.409195\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.661929\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.376004\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.614809\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.337292\n",
      "\n",
      "Test set: Average loss: 0.0076, Accuracy: 9346/10000 (93.46%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.443617\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.299763\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.474277\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.220926\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.381075\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.192868\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.582255\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.455497\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.300899\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.226092\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.354021\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.228802\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.346643\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.296786\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.121209\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.229294\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.387091\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.267360\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.390889\n",
      "\n",
      "Test set: Average loss: 0.0057, Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.340987\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.148632\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.260725\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.815145\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.219620\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.272995\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.224836\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.239478\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.328848\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.092326\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.269890\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.264382\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.347710\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.138337\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.222344\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.273728\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.407689\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.202914\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.287920\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 9572/10000 (95.72%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.147936\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.356995\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.126965\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.291477\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.122926\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.195464\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.113068\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.317988\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.617713\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.246896\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.258319\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.349842\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.144544\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.157720\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.357420\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.121758\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.315285\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.369198\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.169195\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.460604\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.162861\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.448083\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.291652\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.260251\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.322104\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.133354\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.118662\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.612642\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.372269\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.275731\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.306439\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.199274\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.292305\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.181421\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.128105\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.341167\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.416178\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.457235\n",
      "\n",
      "Test set: Average loss: 0.0041, Accuracy: 9616/10000 (96.16%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model\n",
    "def test(model, device, test_loader):\n",
    "    # Sets model in evaluation mode (disables dropout and batch normalization).\n",
    "    model.eval()\n",
    "    # Initialization of cumulative loss and correctly predicted samples.\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Ensures no gradient calcs are performed during the evaluation, reducing memory consumption.\n",
    "    with torch.no_grad():\n",
    "        # Iterates over (image, label) batches in test dataset.\n",
    "        for data, target in test_loader:\n",
    "            # Input data and target labels moved to the GPU/CPU.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass is performed, obtaining predicted outputs.\n",
    "            output = model(data)\n",
    "            # Accumulated test loss between predicted output and target labels.\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Tensor containing the predicted labels by taking the idx of the max value .\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Accumulated correct predictions by comparing predicted labels with target labels.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    # Obtains average loss.\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    # Average loss and accuracy are printed.\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
    "# Runs training process for n-1 epochs.\n",
    "for epoch in range(1, 6):\n",
    "    # Train function called to train the model on training dataset.\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    # Test function called to evaluate models performance on test dataset.\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
